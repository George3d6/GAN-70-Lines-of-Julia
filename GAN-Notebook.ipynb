{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN-70-Lines-of-Julia  \n",
    "This notebook is a demonstration of how to train simple GAN for MNIST by using Julia. It will highlight how one can write a model and related mathematical equations in Julia just like writing a paper. Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We will use Knet.jl deep learning package to compute gradients for the networks and to use GPU arrays if a gpu device available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array{Float32,N} where N"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet,Images, Colors,ImageView\n",
    "include(Pkg.dir(\"Knet\",\"data\",\"mnist.jl\")) #importing MNIST data loader functions\n",
    "global atype = gpu()>=0 ? KnetArray{Float32} : Array{Float32}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Model  \n",
    "We will define a generic multi-layer perceptron to use for discriminator and generator. $\\mathbf{w}$ is an array keeps model paramaters and $\\mathbf{x}$ will be input. Keyword arguments $\\mathbf{p}$ is dropout probability, `activation`  is the activation function used in the hidden layers, `outputactivation` is the activation function used in output layer. We also defined `leakyrelu` activation function since it is not defined in default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leakyrelu (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mlp(w,x;p=0.0,activation=leakyrelu,outputactivation=sigm)\n",
    "    for i=1:2:length(w)\n",
    "        x = w[i]*dropout(mat(x),p) .+ w[i+1]   # mat() used for flatten images to vector.\n",
    "        i<length(w)-1 && (x = activation.(x)) \n",
    "    end\n",
    "    return outputactivation.(x) #output layer\n",
    "end\n",
    "leakyrelu(x;α=Float32(0.2)) = max(0,x) + α*min(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator and Generator Networks\n",
    "\n",
    "Discriminator and Generator networks are simple MLPs named as `D` and `G` in the code. Loss functions `Jd` and `𝑱g` are defined according to equation X in GAN paper. Sample noise function `𝒩` is a normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "𝒩 (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global const 𝜀=Float32(1e-8)\n",
    "D(w,x;p=0.0) = mlp(w,x;p=p)\n",
    "G(w,z;p=0.0) = mlp(w,z;p=p) \n",
    "𝑱d(𝗪d,x,Gz) = -mean(log.(D(𝗪d,x)+𝜀)+log.(1-D(𝗪d,Gz)+𝜀))/2   \n",
    "𝑱g(𝗪g, 𝗪d, z) = -mean(log.(D(𝗪d,G(𝗪g,z))+𝜀))           \n",
    "𝒩(input, batch) = atype(randn(Float32, input, batch))  #SampleNoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Functions\n",
    "\n",
    "For backpropogation we need the derivatives of loss functions according to model parameters. This is where Knet comes to play. The `Knet.grad` function calculates gradient according to first parameter of any function. So,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "∇d  = grad(𝑱d) # Discriminator gradient\n",
    "∇g  = grad(𝑱g) # Generator gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Initialization\n",
    "It is a generic weight initialization function for MLPs. For each layer, it creates a weight matrix and bias vector, then add them to $W$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function initweights(hidden,input, output)\n",
    "    𝗪 = Any[];\n",
    "    x = input\n",
    "    for h in [hidden... output]\n",
    "        push!(𝗪, atype(xavier(h,x)), atype(zeros(h, 1))) #FC Layers weights and bias\n",
    "        x = h\n",
    "    end\n",
    "    return 𝗪  #return model params\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Display\n",
    "\n",
    "This function generates random `number` images and displays here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generate_and_show (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function generate_and_show(𝗪,number,𝞗;fldr=\"generations/\")\n",
    "    Gz = G(𝗪[1],𝒩(𝞗[:ginp],number)) .> 0.5\n",
    "    Gz = permutedims(reshape(Gz,(28,28,number)),(2,1,3))\n",
    "    [display(Gray.(Gz[:,:,i])) for i=1:number]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Test Function\n",
    "\n",
    "This `runmodel` function is implementing training procedure described in GAN paper. It first update discriminator with specified optimizer, then update generator network. Same function can be used in test mode by passing `train` argument as false. In the test mode it calculates losses instead of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "runmodel (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function runmodel(𝗪, data, 𝞗;dtst=nothing,optim=nothing,train=false,saveinterval=10)\n",
    "    gloss=dloss=total=0.0;\n",
    "    B = 𝞗[:batchsize]\n",
    "    for i=1:(train ? 𝞗[:epochs]:1)\n",
    "        for (x,_) in data\n",
    "            Gz = G(𝗪[1],𝒩(𝞗[:ginp],B)) #Generate Fake Images\n",
    "            train ? update!(𝗪[2], ∇d(𝗪[2],x,Gz), optim[2]) : (dloss += 2B*𝑱d(𝗪[2],x,Gz))\n",
    "            \n",
    "            z=𝒩(𝞗[:ginp],2B) #Sample z from Noise\n",
    "            train ? update!(𝗪[1], ∇g(𝗪[1], 𝗪[2], z), optim[1]) : (gloss += 2B*𝑱g(𝗪[1],𝗪[2],z))    \n",
    "            \n",
    "            total+=2B\n",
    "        end\n",
    "        train ? runmodel(𝗪,dtst,𝞗;train=false) : println((gloss/total,dloss/total)) #Print average losses in each epoch\n",
    "        i % saveinterval == 0 && generate_and_show(𝗪,10,𝞗)  # save 10 images\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "𝞗 = Dict(:batchsize=>32,:epochs=>75,:ginp=>256,:genh=>[512],:disch=>[512],:optim=>Adam,:lr=>0.002);\n",
    "xtrn,ytrn,xtst,ytst = mnist()\n",
    "global dtrn = minibatch(xtrn, ytrn, 𝞗[:batchsize]; xtype=atype)\n",
    "global dtst = minibatch(xtst, ytst, 𝞗[:batchsize]; xtype=atype)\n",
    "𝗪  = (𝗪g,𝗪d)   = initweights(𝞗[:genh], 𝞗[:ginp], 784), initweights(𝞗[:disch], 784, 1)\n",
    "𝚶  = (𝚶pg,𝚶pd) =  optimizers(𝗪g,𝞗[:optim];lr=𝞗[:lr]), optimizers(𝗪d,𝞗[:optim];lr=𝞗[:lr])\n",
    "runmodel(𝗪, dtst, 𝞗;optim=𝚶, train=false) # initial losses\n",
    "runmodel(𝗪, dtrn, 𝞗;optim=𝚶,train=true, dtst=dtst) # training  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
